# Emotion-Detection-and-Helper
[사람의 감정을 인식해서 그에 어울리는 사진과 음악을 출력해준다. 4개국어(영어, 한국어, 카자흐어, 러시아어)로 구성되어 있다]  


  이 프로젝트는 사람의 얼굴을 인식해서, 현재 상태를 파악합니다. 그리고 그 감정에 맞는 사진을 출력하고, 음악 들려줍니다. 감정은 총 5개의 class로 나눴습니다: happy, sad, expressionless, surprise, angry.  
  
  
  데이터을 수집 시 구글과 러시아어 기반 검색 엔진 "Yandex"를 사용했습니다. Google에서의 검색 결과는 대부분의 사진들은 대부분 유료 사진이거나 화질이 떨어지는 사진
들이었습니다. 그러나 Yandex에서의 검색 결과는 무료이면서 고화질의 사진들이었습니다. 또한 사진 검색하면서 특정 감정, 특히 슬픔과 분노와 같은 부정적 감정을 나타내는 사진을 찾기 어려웠습니다. 이는 사람들이 부정적인 감정을 숨기려는 심리적 경향성을 반영하는 것으로 보입니다. 대다수의 사람들이 긍정적이고 행복한 모습만을 드러내려는 경향이 있음을 확인할 수 있었습니다. 데이터를 모을 때 최대한 인종, 성별에 맞춰서 해보려고 했습니다.  


  다음은 50장씩 250장 모은 데이터셋입니다.   
  happy : 총 50장 (동양 남자 9, 동양 여자 9, 서양 남자 8, 서양 여자 8, 흑인 남자 7, 흑인 여자 9)  
  sad : 총 50장 (동양 남자 9, 동양 여자 8, 서양 남자 7, 서양 여자 9, 흑인 남자 7, 흑인 여자 10)  
  expressionless : 총 50장 (동양 남자 8, 동양 여자 8, 서양 남자 9, 서양 여자 10, 흑인 남자 7, 흑인 여자 8)  
  surprise : 총 50장 (동양 남자 8, 동양 여자 10, 서양 남자 7, 서양 여자 8, 흑인 남자 8, 흑인 여자 9)  
  angry : 총 50장 (동양 남자 8, 동양 여자 9, 서양 남자 9, 서양 여자 8, 흑인 남자 8, 흑인 여자 8)  
![image](https://github.com/aikozvezda/Emotion-Detection-and-Helper/assets/144213771/af3a7b3e-fd09-49a1-8c7d-da70ddaa94c9) 
![image](https://github.com/aikozvezda/Emotion-Detection-and-Helper/assets/144213771/b2275953-2fb1-4b50-8174-eae966ed0c34)


  근데 여기서 데이터가 적다고 생각해서, 데이터의 양을 augmentation을 이용해서 늘려봤습니다.  
  happy : 352장  
  sad : 347장  
  expressionless : 346장  
  surprise : 344장  
  angry : 346장  
  활용된 사진들은 양이 많아  깃허브에 올리지 못했지만, 주 사진인 250장을 따로따로 파일 안에 저장되어 있습니다.  

  
  그리고 이 데이터셋들을 학습시킬 때 Teachable Machine을 사용했습니다. Teachable Machine은 구글에서 만든 웹기반 노코드 인공지능 학습 툴입니다. 이는 대량의 학습 데이터를 MobileNet으로 사전 훈련 모델을 생성하고, 이 모델의 마지막 일부 레이어만 수정해 전이 학습을 진행하는 방식을 사용합니다. Teachable Machine에서 '모델 내보내기'로 준비된 model을 다운받아서 저의 필요에 따라 코드를 수정했습니다. 그리고 Teachable Machine에서 학습시킬 때 epoch, batch, learning rate를 여러 번 변경해서 학습시켰습니다. 그 이유는 이 세가지 요인에 따라 감정 맞추는 정답률을 조절할 수 있기 때문입니다. 결과적으로 epoch=2000, batch=16, learning rate=0.0005로 설정한 모델로 했습니다.
  model들을 'TMmodels' 파일에 저장되어 있습니다.  

    
  그리고 감정 파악 결과에 따라 나오는 사진이 출력되고, 음악이 재생되도록 했습니다. 이 이유는 사람이 감정에 따라 어울리는 사진과 음악을 준비해 감정을 다스리는 데 도움이 됐으면 해서입니다. 여기서 제가 프로젝트를 이런 주제로 선택한 이유가 있습니다. 주인이 집에 오면 집에 있는 ai가(스마트집) 주인의 감정을 파악해서 그거에 맞는 음악을 틀어주면서 주인에게 심리적으로 도와주는 것입니다. 아니면, 핸드폰에 있는 ai(Siria, 빅스비 등), 자동차안 카메라(자율주행자동차) 외에 많은 부분에서 활용할 수 있습니다. 여기에 쓰인 사진을 ChatGPT를 통해서 만들었습니다. 그리고 음악은 유튜브 채널 'Essential'에서 다운 받아왔습니다. 유튜브 링트는 다음과 같습니다:  
  'happy': 'https://youtu.be/WdBVXXNRRBc?si=FKbyUB1a0FNZRAQZ',  
  'sad': 'https://youtu.be/LlDSMktrlwE?si=phxEPKstJXsIPSV8',  
  'expressionless': 'https://youtu.be/YGX-Y4XngXc?si=VeNar3IpjWhtNB_I',  
  'angry': 'https://youtu.be/Tc0Z6A7xl7Y?si=O3GQAjW9F-lJGsHI',  
  'surprise': 'https://youtu.be/eaqVX9IMACQ?si=Z9i9C4mAsnw2_HMZ'    
  활용된 사진은 '감정.png', 음악은 '감정.mp3'에(음악이 양이 너무 커서 깃허브에 올리지 못했습니다) 저장되어 있습니다.   

   
  프로그램을 작성하면서, 테스트를 저의 얼굴로 했습니다. 하지만, 결과물을 만들 때 따로 테스트 데이트 만들어서 테스팅 했습니다. 테스틑 데이터를 만들기 위해서 class당 사진 1장씩 수집하고, 그 한 사진을 여러가지 필터 씌워서 10장 만들고, 0.8초 간격으로 이어 동영상을 만들었습니다.   
동영상은 'TESTdatavideo'에 저장되어 있습니다. 예시:
https://github.com/aikozvezda/Emotion-Detection-and-Helper/assets/144213771/f3266f42-9653-4fa0-9833-ff5eab5bf89f

   
  그리고 프로젝트를 제가 할 수 있는 4개국어로(영어, 한국어, 카자흐어, 러시아어) 변경할 수 있도록 했습니다.   

   
  결과는 다음과 같은 영상에서 확인할 수 있습니다: 
https://github.com/aikozvezda/Emotion-Detection-and-Helper/assets/144213771/4e4a52a0-1575-49d4-a24d-89829400f9ef

https://github.com/aikozvezda/Emotion-Detection-and-Helper/assets/144213771/7f690569-0894-4de0-8447-6400620f6c45

https://github.com/aikozvezda/Emotion-Detection-and-Helper/assets/144213771/8c45e4ad-a989-47f4-b213-49874f3fa5a2

https://github.com/aikozvezda/Emotion-Detection-and-Helper/assets/144213771/9f78e2a2-f44a-4c94-b1a9-2d19e2249090

https://github.com/aikozvezda/Emotion-Detection-and-Helper/assets/144213771/56c06bc6-9876-416c-911a-472fa6ff2f47

---------------------------------------------------------------------------------------------------------------
   
  코드 내용 설명하겠습니다:   
파이썬 파일 두개 있습니다: 사용자 인터페이스(UI)를 관리하는 파일과 감정 분석을 수행하는 기계 학습 모델 파일입니다.   

   
  main.py - 사용자 인터페이스(UI):   
  tkinter 라이브러리를 사용하여 사용자 인터페이스를 구축합니다. 웹캠을 통해 실시간으로 사용자의 영상을 캡처하고, 이를 5초후에 1초 간격으로 10초 동안 캡처를 해서, test_data 10장을 만들어냅니다. 사용자의 언어 선택에 따라 UI의 텍스트를 변경할 수 있는 기능을 포함합니다. analyze_emotions 함수를 호출하여 사용자의 감정을 분석하고, 결과를 화면에 표시합니다. 배경음악을 제어하고, 감정에 따라 다른 사진 표시와 음악을 재생할 수 있는 기능도 포함합니다.    


  model.py - 학습 모델 활용   
  keras 라이브러리를 사용하여 Teachable Machine에서 다운받은 감정 분석 모델을 로드합니다. 모델은 'test_data'에 파일에 저장된, main.py에서 만들어낸 10장의 사진, 224x224 픽셀 크기의 사진를 입력으로 받아, 사진에서 감정을 분석합니다. analyze_emotion 함수는 개별 이미지에 대해 감정을 분석하고, 해당 감정과 분석의 신뢰도(정확도)를 반환합니다.
analyze_emotions 함수는 여러 이미지를 분석하고 가장 흔한 감정을 결정합니다.    

  
  프로젝트의 실행 과정은 다음과 같습니다:  
  -사용자가 애플리케이션을 시작하고, 웹캠을 통해 자신의 얼굴을 보여줍니다.  
  -사용자가 원하는 언어를 선택하면, UI의 모든 텍스트가 해당 언어로 변경됩니다.  
  -애플리케이션은 사용자의 감정을 분석하기 위해 10장의 사진을 캡처합니다.  
  -캡처된 사진들은 모델에 의해 분석되며, 가장 흔한 감정이 화면에 표시됩니다.  
  -감정에 따라,사진이 표시되고 음악이 재생되며, 사용자는 이를 제어할 수 있습니다.  

  
  이 프로젝트는 인공지능과 컴퓨터 비전을 활용하여 사용자의 감정 상태를 이해하고, 이를 사용자에게 피드백하는 현대적인 접근 방식을 사용합니다. 이런 유형의 애플리케이션은 사용자 경험을 향상시키고, 감정 기반의 인터랙티브한 시나리오를 가능하게 합니다. 예를 들어, 온라인 교육 플랫폼에서 학생의 감정을 모니터링하여 학습 경험을 개인화하거나, 게임에서 플레이어의 반응에 따라 게임의 난이도를 조절하는 등 다양한 분야에서 응용할 수 있습니다.  


  하지만, 사람의 감정을 파악하는 것이 사람에게도 어렵습니다. 이 프로젝트를 하면서 저에게 행복해보이는 표정이 다른 사람에게 무표정으로 느껴질 수 있고, 다른 사람에게 슬픈 표정이 저에게 행복한 표정으로 보일수도 있다는 것을 알게 되었습니다. 아직까지도 사람의 표정을 완벽하게 분석하는 인공지능 기술이 없는 것 같습니다. 저 또한 이번 프로젝트에서 최대한 결과가 잘 나올 것 같은 사진으로 골라서 사용했습니다. 하지만, 카메라 앞에 다른 준비된 사진이 아닌 제가 나오게 되면, 아무리 표정을 과하게 지어도 대부분을 happy로 인식하는 경우가 많았습니다. 그래도 표정분석이 해결되면, 이 세상의 많은 문제들이 해결될 수 있을 것 같습니다.  
